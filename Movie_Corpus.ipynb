{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Natural Language Processing with NLTK\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import jaccard_distance, edit_distance\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the plot summaries text\n",
    "with open('plots.txt', 'rt', encoding=\"utf8\") as file:\n",
    "    raw_plots_text = file.read()\n",
    "\n",
    "# Tokenize the raw plots into word tokens\n",
    "tokens = nltk.word_tokenize(raw_plots_text)\n",
    "text_tokens = nltk.Text(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# PART 1 - TEXT ANALYSIS\n",
    "# -------------------------\n",
    "\n",
    "# 1. Count total number of tokens\n",
    "def get_total_token_count():\n",
    "    return len(tokens)\n",
    "\n",
    "# 2. Count number of unique tokens\n",
    "def get_unique_token_count():\n",
    "    return len(set(tokens))\n",
    "\n",
    "# 3. Count number of unique lemmatized verbs\n",
    "def get_unique_lemmatized_verbs():\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmatized_verbs = [lemmatizer.lemmatize(w, 'v') for w in text_tokens]\n",
    "    return len(set(lemmatized_verbs))\n",
    "\n",
    "# 4. Calculate lexical diversity: unique tokens / total tokens\n",
    "def calculate_lexical_diversity():\n",
    "    return len(set(tokens)) / len(tokens)\n",
    "\n",
    "# 5. Percentage of 'love' or 'Love' in the text\n",
    "def percentage_of_love_words():\n",
    "    count_love = sum(1 for w in tokens if w in ['love', 'Love'])\n",
    "    return (count_love / len(tokens)) * 100\n",
    "\n",
    "# 6. Top 20 most frequent tokens\n",
    "def get_top_20_tokens():\n",
    "    freq_dist = Counter(tokens)\n",
    "    return freq_dist.most_common(20)\n",
    "\n",
    "# 7. Tokens longer than 5 characters and occur more than 200 times\n",
    "def frequent_long_tokens():\n",
    "    freq_dist = Counter(tokens)\n",
    "    filtered = [token for token, count in freq_dist.items() if len(token) > 5 and count > 200]\n",
    "    return sorted(filtered)\n",
    "\n",
    "# 8. Longest token in the text and its length\n",
    "def find_longest_token():\n",
    "    longest = max(text_tokens, key=len)\n",
    "    return (longest, len(longest))\n",
    "\n",
    "# 9. Unique words with frequency > 2000 (alphabetic only)\n",
    "def highly_frequent_words():\n",
    "    freq_dist = Counter(tokens)\n",
    "    filtered = [(count, word) for word, count in freq_dist.items() if word.isalpha() and count > 2000]\n",
    "    return sorted(filtered, reverse=True)\n",
    "\n",
    "# 10. Average number of tokens per sentence\n",
    "def avg_tokens_per_sentence():\n",
    "    text_str = \" \".join(text_tokens)\n",
    "    sentences = nltk.sent_tokenize(text_str)\n",
    "    return len(tokens) / len(sentences)\n",
    "\n",
    "# 11. Top 5 part-of-speech tags\n",
    "def top_pos_tags():\n",
    "    pos_tags = nltk.pos_tag(text_tokens)\n",
    "    tag_counts = Counter(tag for word, tag in pos_tags)\n",
    "    return tag_counts.most_common(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# PART 2 - SPELLING RECOMMENDERS\n",
    "# -------------------------\n",
    "\n",
    "# 12. Recommender using Jaccard distance on trigrams\n",
    "def spelling_recommender_jaccard_trigrams(misspelled=['cormulent', 'incendenece', 'validrate']):\n",
    "    suggestions = []\n",
    "    for word in misspelled:\n",
    "        candidates = [w for w in words.words() if w.startswith(word[0]) and len(w) >= 3]\n",
    "        distances = [(w, jaccard_distance(set(ngrams(word, 3)), set(ngrams(w, 3)))) for w in candidates]\n",
    "        best = min(distances, key=lambda x: x[1])[0]\n",
    "        suggestions.append(best)\n",
    "    return suggestions\n",
    "\n",
    "# 13. Recommender using Jaccard distance on 4-grams\n",
    "def spelling_recommender_jaccard_fourgrams(misspelled=['cormulent', 'incendenece', 'validrate']):\n",
    "    suggestions = []\n",
    "    for word in misspelled:\n",
    "        candidates = [w for w in words.words() if w.startswith(word[0]) and len(w) >= 4]\n",
    "        distances = [(w, jaccard_distance(set(ngrams(word, 4)), set(ngrams(w, 4)))) for w in candidates]\n",
    "        best = min(distances, key=lambda x: x[1])[0]\n",
    "        suggestions.append(best)\n",
    "    return suggestions\n",
    "\n",
    "# 14. Recommender using Edit Distance (Levenshtein with transpositions)\n",
    "def spelling_recommender_edit_distance(misspelled=['cormulent', 'incendenece', 'validrate']):\n",
    "    suggestions = []\n",
    "    for word in misspelled:\n",
    "        candidates = [w for w in words.words() if w.startswith(word[0])]\n",
    "        distances = [(w, edit_distance(word, w, transpositions=True)) for w in candidates]\n",
    "        best = min(distances, key=lambda x: x[1])[0]\n",
    "        suggestions.append(best)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 374446\n",
      "Unique Tokens: 25928\n",
      "Unique Lemmatized Verbs: 21755\n",
      "Lexical Diversity: 0.06924362925495264\n",
      "Love Word %: 0.12391639916035956\n",
      "Top 20 Tokens: [(',', 19420), ('the', 18698), ('.', 16629), ('to', 12149), ('and', 11400), ('a', 8979), ('of', 6510), ('is', 5699), ('in', 5109), ('his', 4693), (\"'s\", 3682), ('her', 3674), ('he', 3556), ('that', 3517), ('with', 3293), ('him', 2570), ('for', 2433), ('by', 2321), ('The', 2234), ('on', 1925)]\n",
      "Frequent Long Tokens: ['However', 'Meanwhile', 'another', 'because', 'becomes', 'before', 'begins', 'daughter', 'decides', 'escape', 'family', 'father', 'friend', 'friends', 'himself', 'killed', 'leaves', 'mother', 'people', 'police', 'returns', 'school', 'through']\n",
      "Longest Token: ('live-for-today-for-tomorrow-we-die', 34)\n",
      "Highly Frequent Words: [(18698, 'the'), (12149, 'to'), (11400, 'and'), (8979, 'a'), (6510, 'of'), (5699, 'is'), (5109, 'in'), (4693, 'his'), (3674, 'her'), (3556, 'he'), (3517, 'that'), (3293, 'with'), (2570, 'him'), (2433, 'for'), (2321, 'by'), (2234, 'The')]\n",
      "Avg Tokens per Sentence: 22.230230349085726\n",
      "Top POS Tags: [('NN', 51450), ('IN', 39225), ('NNP', 38360), ('DT', 34471), ('VBZ', 23799)]\n",
      "Recommender (Jaccard Trigram): ['corpulent', 'indecence', 'validate']\n",
      "Recommender (Jaccard 4-gram): ['cormus', 'incendiary', 'valid']\n",
      "Recommender (Edit Distance): ['corpulent', 'intendence', 'validate']\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Tokens:\", get_total_token_count())\n",
    "print(\"Unique Tokens:\", get_unique_token_count())\n",
    "print(\"Unique Lemmatized Verbs:\", get_unique_lemmatized_verbs())\n",
    "print(\"Lexical Diversity:\", calculate_lexical_diversity())\n",
    "print(\"Love Word %:\", percentage_of_love_words())\n",
    "print(\"Top 20 Tokens:\", get_top_20_tokens())\n",
    "print(\"Frequent Long Tokens:\", frequent_long_tokens())\n",
    "print(\"Longest Token:\", find_longest_token())\n",
    "print(\"Highly Frequent Words:\", highly_frequent_words())\n",
    "print(\"Avg Tokens per Sentence:\", avg_tokens_per_sentence())\n",
    "print(\"Top POS Tags:\", top_pos_tags())\n",
    "print(\"Recommender (Jaccard Trigram):\", spelling_recommender_jaccard_trigrams())\n",
    "print(\"Recommender (Jaccard 4-gram):\", spelling_recommender_jaccard_fourgrams())\n",
    "print(\"Recommender (Edit Distance):\", spelling_recommender_edit_distance())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
